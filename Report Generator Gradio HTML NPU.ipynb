{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8098475b-6ebe-441e-b12f-fdb628c5d563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import jinja2\n",
    "import pdfkit\n",
    "import torch\n",
    "import transformers\n",
    "import warnings\n",
    "\n",
    "from intel_npu_acceleration_library import NPUModelForCausalLM, int4\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb7af900-bc87-4d98-909e-e2ea332c5326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load CSV database\n",
    "def load_csv(file):\n",
    "    df = pd.read_csv(file.name)\n",
    "    return df\n",
    "\n",
    "# Step 1.1\n",
    "def convert_binary_columns(df):\n",
    "    for col in df.columns:\n",
    "        if set(df[col].dropna().unique()) <= {0, 1}:  # Check if column is binary (only 0s and 1s)\n",
    "            df[col] = df[col].map({1: 'Yes', 0: 'No'})  # Convert to Yes/No\n",
    "    return df\n",
    "\n",
    "#Step 1.2\n",
    "def generate_data_summaries(df):\n",
    "    # Numerical summary (after binary conversion)\n",
    "    numerical_summary = df.describe().to_string()\n",
    "\n",
    "    # Categorical summary\n",
    "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "    categorical_summary = {col: df[col].value_counts().to_string() for col in categorical_columns}\n",
    "    \n",
    "    return numerical_summary, categorical_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13dd669f-040d-4faf-b1f0-5a49cd42adc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IntelDell\\Desktop\\AI\\Report-Generator-LLM-on-Intel-CPU-NPU\\langchain-env\\lib\\site-packages\\intel_npu_acceleration_library\\modelling.py:98: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(model_path)\n"
     ]
    }
   ],
   "source": [
    "#Step 2: Option NPU - Set up local LLM model (using huggingface pipeline)\n",
    "model_id = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "model = NPUModelForCausalLM.from_pretrained(model_id, use_cache=True, dtype=int4).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_default_system_prompt=True)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "#tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "#prefix = tokenizer(messages, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "generation_kwargs = dict(\n",
    "    #input_ids=prefix,\n",
    "    streamer=streamer,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "# Step 3: Create the HF pipe\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=2048,streamer=streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d9641c7-3890-424b-b93a-d193040dc50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4.1: Generate final user prompt\n",
    "def generate_response(user_prompt, numerical_summary, categorical_summary):\n",
    "    # Prepare the input for the model\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"You are an expert business and data analyst. Using the details provided in the numerical summary and categorical summary, generate a detailed business analysis report based on the user's question.\n",
    "    Your report should include a report title, introduction, methodology, description of the data provided by user, findings and insights, detailed and elaborate analysis supported by analytical evidence, recommendations and a closing summary.\n",
    "    Make sure the response is focused and avoid unnecessary repetition. Your response should be exclusively provided in English language.\n",
    "    End your response once you have provided a closing summary. Notes or disclaimers are not required.\n",
    "    Provide your report in a HTML document format following the rules as listed below:\n",
    "        - <h2> tags For headers and <h4> tags for sub-headers\n",
    "        - <ul> tags for unordered listings and <ol> for ordered listings\n",
    "        - <b> tag for bolding of text\n",
    "        - Follow all the other HTML syntax\n",
    "    \"\"\"\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"Numerical Summary: {numerical_summary}\n",
    "        Categorical Summary: {categorical_summary}\n",
    "        Generate a business report based on the following question\n",
    "        \n",
    "        Question: {user_prompt}\"\"\"\n",
    "    }]\n",
    "\n",
    "    return messages\n",
    "\n",
    "#Step 4.2 Function to generate report from the DataFrame\n",
    "def generate_report_from_dataframe(user_prompt, file):\n",
    "    # Load the CSV filepath\n",
    "    df = pd.read_csv(file.name)\n",
    "    \n",
    "    # Step to convert binary columns to Yes/No\n",
    "    df = convert_binary_columns(df)\n",
    "\n",
    "    # Generate summaries\n",
    "    numerical_summary, categorical_summary = generate_data_summaries(df)\n",
    "    categorical_summary = \"\\n\".join(f\"{key}:\\n{value}\" for key, value in categorical_summary.items())\n",
    "\n",
    "    # Generate the report using the LLM\n",
    "    messages = generate_response(user_prompt, numerical_summary, categorical_summary)\n",
    "    output = pipe(messages, **generation_kwargs)\n",
    "\n",
    "    if isinstance(output, list) and 'generated_text' in output[0]:\n",
    "        for message in output[0]['generated_text']:\n",
    "            if message.get('role') == 'assistant':\n",
    "                report = message.get('content', '').strip()\n",
    "                break\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d23b8b4-c099-41d4-938d-cf526ce46aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Function to convert html report output to pdf \n",
    "def convert_html_to_pdf(\n",
    "    html_string: str,\n",
    "    template_folder: str = \"./templates\",\n",
    "    base_html_file: str = \"base.html\",\n",
    "    output_file: str = \"generate_pdf.pdf\",\n",
    "    output_folder: str = \"./generated_pdf_files\",\n",
    ") -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    Asynchronously converts an HTML file to a PDF file.\n",
    "\n",
    "    This function takes the path of an HTML template file and generates a PDF file from it. The HTML file is expected to be located in the specified 'template_folder'. The generated PDF is saved with the name provided in 'output_file' and is stored in the specified 'output_folder'.\n",
    "\n",
    "    Parameters:\n",
    "    template_folder (str, optional): The path to the folder containing the HTML template. Defaults to \"./templates\".\n",
    "    base_html_file (str, optional): The name of the base HTML file within the template folder. Defaults to \"base.html\".\n",
    "    output_file (str, optional): The name of the output PDF file. Defaults to \"generate_pdf.pdf\".\n",
    "    output_folder (str, optional): The path to the folder where the generated PDF file will be saved. Defaults to \"../generated_pdf_files\".\n",
    "\n",
    "    Returns:\n",
    "    str: The path to the generated PDF file.\n",
    "\n",
    "    Note:\n",
    "    - This function is asynchronous and should be awaited upon calling.\n",
    "    - Ensure that the specified folders and files exist and are accessible.\n",
    "    - The function might raise exceptions related to file reading/writing or PDF generation which should be handled appropriately.\n",
    "    \"\"\"\n",
    "\n",
    "    if output_folder.endswith(\"/\"):\n",
    "        raise ValueError(\"Wrong output folder name, should not end with '/'\")\n",
    "    else:\n",
    "        pdf_file_name = f\"{output_folder}/{output_file}\"\n",
    "\n",
    "    try:\n",
    "        template_loader = jinja2.FileSystemLoader(template_folder)\n",
    "        template_env = jinja2.Environment(loader=template_loader)\n",
    "\n",
    "        basic_template = template_env.get_template(base_html_file)\n",
    "\n",
    "        output_html_code = basic_template.render()\n",
    "        # print(output_html_code)\n",
    "\n",
    "        # render content, this if for once we have AI generated response\n",
    "        output_html_code = basic_template.render(\n",
    "            ai_generated_content=html_string\n",
    "        )\n",
    "\n",
    "        options = {\n",
    "            'page-size': 'A4',\n",
    "            'margin-top': '0.75in',\n",
    "            'margin-bottom': '0.75in',\n",
    "            'margin-right': '0.55in',\n",
    "            'margin-left': '0.55in',\n",
    "            'encoding': \"UTF-8\",\n",
    "            'footer-right': '[page] of [topage]',\n",
    "            'footer-font-size': \"9\",\n",
    "            'custom-header': [\n",
    "                ('Accept-Encoding', 'gzip')\n",
    "            ],\n",
    "            'enable-local-file-access': False,\n",
    "            'no-outline': None,\n",
    "            'enable-local-file-access': False,\n",
    "            'no-outline': None\n",
    "        }\n",
    "\n",
    "        config = pdfkit.configuration(wkhtmltopdf=\"C:\\\\Program Files\\\\wkhtmltopdf\\\\bin\\\\wkhtmltopdf.exe\")\n",
    "\n",
    "        pdfkit.from_string(\n",
    "            input=output_html_code,\n",
    "            output_path=pdf_file_name,\n",
    "            options=options,\n",
    "            configuration=config\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        # good to log this exception instead\n",
    "        print(e)\n",
    "        return \"\"\n",
    "\n",
    "    return f\"PDF generated successfully at {pdf_file_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be4001b-93f3-476c-ab43-d68b4801d8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/07 17:59:05 [W] [service.go:132] login to server failed: dial tcp 44.237.78.176:7000: i/o timeout\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Main Reasons for Customer Churn and Related Insights\n",
      "\n",
      "Executive Summary:\n",
      "\n",
      "The objective of this report is to analyze and understand the three main reasons for customer churn within our business or any relevant context. For the analysis, we will examine provided categorical and numerical data related to various business aspects such as card ownership, demographics, payment history, etc. This will enable us to identify patterns and correlations which can be translated into insights to combat customer churn.\n",
      "\n",
      "Step one: Organize and prepare data\n",
      "-------------------------\n",
      "To begin, we shall merge, sort, and clean the data to prepare it for analysis, removing any outliers or duplicates that may bias our results.\n",
      "\n",
      "Step two: Numerical and Categorical Data Examination\n",
      "---------------------------\n",
      "We will examine the numerical (age, balance, payment history, etc.) and categorical (country, gender, card type, etc.) data. This examination will involve creating visual tools like histogoks (for numerical data) and bar charts (for categorical data).\n",
      "\n",
      "Step three: Hypothesize on the reasons for Customer Churn\n",
      "----------------------------\n",
      "Based on the initial data, we form hypotheses on the possible reasons for customer churn. For instance:\n",
      "\n",
      "1. Balance Scarcity: Perhaps customers with consistently low balances are more likely to churn, possibly because they use the card less frequently.\n",
      "\n",
      "2. Credit Limit: Customers may be missing out on their full card benefits (rewards, interest-free period) due to low usage, "
     ]
    }
   ],
   "source": [
    "with gr.Blocks(theme=gr.Theme.from_hub('HaleyCH/HaleyCH_Theme'), css=\".column-form .wrap {flex-direction: column;}\") as app:\n",
    "    with gr.Row():\n",
    "        gr.Markdown(\"\"\"<h1><center>Report Generator</center>\"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(visible=True, min_width=350, scale=0) as sidebar:\n",
    "            with gr.Row():\n",
    "                # CSV file uploader\n",
    "                csv_file_input = gr.File(label=\"Upload your CSV file\", file_types=[\".csv\"])\n",
    "\n",
    "\n",
    "            with gr.Row():\n",
    "                # Textbox for user prompt\n",
    "                user_prompt_input = gr.Textbox(\n",
    "                    label=\"Enter focus area for analysis\",\n",
    "                    placeholder=\"e.g., What are the 3 main reasons for customer churn?\\nPress `Enter` to generate report\",\n",
    "                    lines=1\n",
    "                )\n",
    "\n",
    "            with gr.Row():\n",
    "                generate_pdf = gr.Button(\n",
    "                    value=\"Press to convert report to PDF\"\n",
    "                )\n",
    "        \n",
    "        with gr.Column() as main:\n",
    "            with gr.Row():\n",
    "                # Output area for the generated report\n",
    "                output_html = gr.HTML(label=\"Generated Report\")\n",
    "    \n",
    "    # Set up the action to happen when the button is clicked\n",
    "    user_prompt_input.submit(\n",
    "        fn=generate_report_from_dataframe,\n",
    "        inputs=[user_prompt_input, csv_file_input],\n",
    "        outputs=output_html\n",
    "        #outputs=textbox\n",
    "    )\n",
    "\n",
    "    # Convert HTML to PDF and display info message\n",
    "    generate_pdf.click(\n",
    "        fn=lambda html_content: (\n",
    "        convert_html_to_pdf(html_content),\n",
    "        gr.Info(convert_html_to_pdf(html_content))\n",
    "        ),\n",
    "        inputs=output_html,\n",
    "        outputs=None\n",
    "    )    \n",
    "\n",
    "# Launch the Gradio app\n",
    "app.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9150453c-d81f-4093-b7a8-4825a2112628",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
